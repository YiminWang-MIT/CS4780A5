\documentclass[letterpaper,11pt]{article}
\title{CS 4780 Assignment 5}
\author{Feiran Chen(fc254), Yimin Wang(yw475)}
\date{\today}
\usepackage{amssymb}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}
\maketitle
\section{Problem 1: Viterbi Algorithm}
\subsection{}
In dynamic programming, the probability of current state only depends on the
previous step.
So the recurence relation of $\delta_{s,t}$ is:
\[\delta_{s,t}=\argmax_{y_{t-1} \in \{a,i,p,s\}}\delta_{y_{t-1},t-1}P(x_t|s)P(s|y_{t-1})\]
And the initial conditions are:
\[\delta_{s,1}=P(s)P(x_1|s)\]
\subsection{}

\begin{tabular}{l | r r}
  \multicolumn{3}{l}{For word $\eta \tau $ we have:}\\
  $\delta$ & t=1 & t=2 \\
  \hline
  s=1 & 0.0100, -1 & 0.0032, 1 \\
  s=2 & 0.1600, -1 & 0.0008, 1 \\
  s=3 & 0.0200, -1 & 0.0040, 1 \\
  s=4 & 0.0300, -1 & 0.0384, 1 \\
  \multicolumn{3}{l}{So the most likely English translation is \textbf{is}.}\\
\end{tabular}\\
\newline
\newline
\begin{tabular}{l | r r r}
  \multicolumn{4}{l}{For word $\gamma \alpha \omega$ we have:}\\
  $\delta$ & t=1 & t=2 & t=3 \\
  \hline
  s=1 & 0.02000, -1 & 0.01260, 3 & 0.00009, 3 \\
  s=2 & 0.04000, -1 & 0.00540, 3 & 0.00013, 0 \\
  s=3 & 0.04000, -1 & 0.00135, 3 & 0.00094, 0 \\
  s=4 & 0.09000, -1 & 0.00270, 3 & 0.00088, 0 \\
  \hline
  \multicolumn{4}{l}{So the most likely English translation is \textbf{sap}.}\\
\end{tabular}\\
\newline
\newline
\begin{tabular}{l | r r r r}
  \multicolumn{5}{l}{For word $\omega \alpha \gamma \tau$ we have:}\\
  $\delta$ & t=1 & t=2 & t=3 & t=4 \\
  \hline
  s=1 & 0.0100, -1 & 0.0180, 2 & 0.0002, 3 & 0.0003, 3 \\
  s=2 & 0.0400, -1 & 0.0045, 2 & 0.0002, 0 & 0.0001, 3 \\
  s=3 & 0.1000, -1 & 0.0010, 1 & 0.0005, 0 & 0.0001, 3 \\
  s=4 & 0.0300, -1 & 0.0035, 2 & 0.0038, 0 & 0.0005, 3 \\
  \multicolumn{5}{l}{So the most likely English translation is \textbf{pass}.}\\
\end{tabular}
\newline
Therefore, Ian's English password is \textbf{pass is sap}.

\subsection{}

Given the assumption that all reading takes only a constant time,
the running time for Viterbi algorithm is $\mathcal{O}(Tm^2)$, while the
brute-force algorithm has running time of $\mathcal{O}(Tm^T)$.

\subsection{}
Given all characters in English as set $\mathcal{V}$,
we know that:
\begin{equation}
  P(X=(x_1,x_2,\cdots,x_T))=\sum_{(y_1,y_2,\cdots,y_T) \in \mathcal{V}^T}P
\left((x_1,x_2,\cdots,x_T)|(y_1,y_2,\cdots,y_T))\right)
\end{equation}
We also know that for each set of $(y_1,y_2,\cdots,y_T)$:\\
\begin{equation}
P\left((x_1,x_2,\cdots,x_T)|(y_1,y_2,\cdots,y_T))\right)=P(y_1)P(x_1|y_1)\prod_{t=2}^TP(x_t|y_t)P(y_t|y_{t-1})
\end{equation}
So we have:
\begin{equation}
  P(X=(x_1,x_2,\cdots,x_T))=\sum_{(y_1,y_2,\cdots,y_T) \in \mathcal{V}^T}P(y_1)P(x_1|y_1)\prod_{i=2}^TP(x_i|y_i)P(y_i|y_{i-1})
\end{equation}
Notice is using brute-force for each terms in the summation, the complexity is
$\mathcal{O}(Tm^T)$.

Once again, we can use dynamic programming algorithm. Let $\delta_{s,t}'$ as the
probability or the word $(x_1,x_2,\cdots,x_t)$ whose tranlstion in
English ends with character $y_s$.

The recurence relation of $\delta_{s,t}'$ is:
\[\delta_{s,t}'=\sum_{y_{t-1} \in \mathcal{V}}\delta_{y_{t-1},t-1}P(x_t|s)P(s|y_{t-1})\]
And the initial conditions are:
\[\delta_{s,1}'=P(s)P(x_1|s)\]

So the final probability for word $P\left( X=(x_1,x_2,\cdots,x_T) \right) =
\sum_{s \in \mathcal{V}}\delta_{y_s,T}'$
The running time for this dynamic algorithm is $\mathcal{O}(Tm^2)$.

\subsection{}
We don't think HMMs will work well in translating sentences. Sentences in
different languages will have significant differences in structures and
therefore there may not be one-to-one correspondence in words of the same
sentence. Also in sentences, words that are far apart from each other may have
strong correlations sometimes. The Markov Model will fail in both of these cases.

\section{Problem 2: Statistical Learning Theory}
\subsection{}
For all these hypotheses classes, empty set or full set are trivial.
\begin{description}
  \item[Interval of the reals] \hfill \\
    For any three points $\{a,b,c (a<b<c)\}$ in $\mathbb{R}$ .
    One interval can seperate any point in the set from the other two and by labeling the
    interval with either $+$ or $-$ the sets of points can be shattered.
    So $VC_{dim}(H) \ge 3$.
  \item[Pairs of intervals of the reals]\hfill \\
    For any ordered five points $\{x_1,\cdots, x_5\}$ in $\mathbb{R}$ .
    Two intervals can seperate any two of the points from the other three. Or
    one intervals can seperate one point from the other four. And by labeling the
    interval with either $+$ or $-$ the sets of points can be shattered.
    So $VC_{dim}(H) \ge 5$.
  \item[Rectangular hypotheses centered at the origin]\hfill \\
    For three points $\{(2,2),(3,0),(0,-4)\}$. Rectangulars
    $\{[2.5,2.5],[4,1],[1,5]\}$ can seperate each one of the points from the other
    two. And by labeling the inside with either $+$ or $-$ the sets of points can be shattered.
    So $VC_{dim}(H) \ge 3$.
  \item[Rectangular hypotheses centered arbitrarily]\hfill \\
    For four points $\{(0,0),(1,6),(2,-3),(-7,-1)\}$. Small rectangulars
    centered at each point can seperate that point from other points.
    Rectangulars $\{[-1,2,-1,7],[-1,3,-4,1],[-8,1,-2,1]\}$ can seperate 
    each two of the points from the other two. And by labeling the inside 
    with either $+$ or $-$ the sets of points can be shattered.
    So $VC_{dim}(H) \ge 4$.
  \item[Circular hypotheses centered arbitrarily]\hfill \\
    For four points $\{(0,0),(2,0),(-2,0),(0,2)\}$. Small circles 
    centered at each point can seperate that point from other points.
    Circles $\{[r=1.5,c=(0,1)],[r=1.5,c=(1,0)],[r=1.5,c=(-1,0)]\}$ can seperate 
    each two of the points from the other two. And by labeling the inside 
    with either $+$ or $-$ the sets of points can be shattered.
    So $VC_{dim}(H) \ge 4$.

\end{description}
\subsection{}
The size of the instance space is $2^n$. Each instance can have two different
values from the function. 
Therefore the size of the hypothesis space
$|H_n|=2^{2^n}$.
For any self-consistent set, $H_n$ can shatter the set. Therefore,
$VC_{dim}(H_n) \ge 2^n$.
\subsection{}
Each $w_i$ can be chosen from two values and $b$ can be chosen from 31 different values.
So $|H|=31*2^{100}$.
From the Error bound formula derived in the lecture.
\begin{equation}
  P\left( |Err_s(h_{L(s)})-Err_p(h_{L(s)})|\ge \epsilon \right)\le
  |H|e^{-n\epsilon}=31e^{100ln(2)-n\epsilon}=\delta
\end{equation}
Therefore we have $\delta=31e^{100ln(2)-n\epsilon} \rightarrow
\epsilon=\frac{100ln(2)-ln(\delta/31)}{n}$
\subsection{}
\begin{equation}
  P\left( |Err_s(\hat{h}_{s})-Err_p(\hat{h}_{s})|\le \delta \right)\ge
  1-2|H|e^{-2n\delta^2}=1-62e^{100ln(2)-2n\delta^2}
\end{equation}
So we have:
$62e^{100ln(2)-2n*0.1^2}=0.1 \Rightarrow n=3788$

\end{document}

